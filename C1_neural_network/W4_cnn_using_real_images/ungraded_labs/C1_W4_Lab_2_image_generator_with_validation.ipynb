{"cells":[{"cell_type":"markdown","metadata":{"id":"eHBkAdENa5FY"},"source":["<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C1/W4/ungraded_labs/C1_W4_Lab_2_image_generator_with_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"xB2cQUShkXNm"},"source":["# Ungraded Lab: ImageDataGenerator with a Validation Set\n","\n","In this lab, you will continue using the `ImageDataGenerator` class to prepare the `Horses or Humans` dataset. This time, you will add a validation set so you can also measure how well the model performs on data it hasn't seen."]},{"cell_type":"markdown","metadata":{"id":"WsO-u_3fySMd"},"source":["**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running it on your local machine might result in some of the code blocks throwing errors."]},{"cell_type":"markdown","metadata":{"id":"l5FfBGV5yUjb"},"source":["Run the code blocks below to download the datasets `horse-or-human.zip` and `validation-horse-or-human.zip` respectively."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"RXZT2UsyIVe_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704139424795,"user_tz":360,"elapsed":1755,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"8f3b966b-a85b-4b51-db88-e8fffe767cd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-01-01 20:03:43--  https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.207, 172.253.117.207, 142.250.99.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘horse-or-human.zip’\n","\n","horse-or-human.zip  100%[===================>] 142.65M   103MB/s    in 1.4s    \n","\n","2024-01-01 20:03:44 (103 MB/s) - ‘horse-or-human.zip’ saved [149574867/149574867]\n","\n"]}],"source":["# Download the training set\n","!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0mLij6qde6Ox","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704139445462,"user_tz":360,"elapsed":397,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"d64458df-b3d1-4974-c626-c5da86ee8707"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-01-01 20:04:05--  https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.207, 172.253.117.207, 142.250.99.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11480187 (11M) [application/zip]\n","Saving to: ‘validation-horse-or-human.zip’\n","\n","\r          validatio   0%[                    ]       0  --.-KB/s               \rvalidation-horse-or 100%[===================>]  10.95M  --.-KB/s    in 0.1s    \n","\n","2024-01-01 20:04:05 (79.8 MB/s) - ‘validation-horse-or-human.zip’ saved [11480187/11480187]\n","\n"]}],"source":["# Download the validation set\n","!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"]},{"cell_type":"markdown","metadata":{"id":"9brUxyTpYZHy"},"source":["Then unzip both archives."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PLy3pthUS0D2","executionInfo":{"status":"ok","timestamp":1704139449274,"user_tz":360,"elapsed":842,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}}},"outputs":[],"source":["import zipfile\n","\n","# Unzip training set\n","local_zip = './horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./horse-or-human')\n","\n","# Unzip validation set\n","local_zip = './validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./validation-horse-or-human')\n","\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"o-qUPyfO7Qr8"},"source":["Similar to the previous lab, you will define the directories containing your images. This time, you will include those with validation data."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NR_M9nWN-K8B","executionInfo":{"status":"ok","timestamp":1704139450659,"user_tz":360,"elapsed":6,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}}},"outputs":[],"source":["import os\n","\n","# Directory with training horse pictures\n","train_horse_dir = os.path.join('./horse-or-human/horses')\n","\n","# Directory with training human pictures\n","train_human_dir = os.path.join('./horse-or-human/humans')\n","\n","# Directory with validation horse pictures\n","validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n","\n","# Directory with validation human pictures\n","validation_human_dir = os.path.join('./validation-horse-or-human/humans')"]},{"cell_type":"markdown","metadata":{"id":"LuBYtA_Zd8_T"},"source":["Now see what the filenames look like in these directories:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4PIP1rkmeAYS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704139452751,"user_tz":360,"elapsed":7,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"0e9ee0eb-32dc-496d-b45d-314a7d527e9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN SET HORSES: ['horse44-0.png', 'horse48-6.png', 'horse44-4.png', 'horse41-8.png', 'horse02-0.png', 'horse36-5.png', 'horse03-9.png', 'horse10-8.png', 'horse28-5.png', 'horse49-4.png']\n","TRAIN SET HUMANS: ['human10-17.png', 'human04-02.png', 'human17-10.png', 'human02-29.png', 'human08-21.png', 'human07-07.png', 'human02-01.png', 'human17-17.png', 'human09-22.png', 'human15-01.png']\n","VAL SET HORSES: ['horse2-011.png', 'horse5-235.png', 'horse3-198.png', 'horse1-264.png', 'horse5-478.png', 'horse6-198.png', 'horse1-276.png', 'horse1-127.png', 'horse1-122.png', 'horse3-469.png']\n","VAL SET HUMANS: ['valhuman05-13.png', 'valhuman03-11.png', 'valhuman03-19.png', 'valhuman01-23.png', 'valhuman02-16.png', 'valhuman02-18.png', 'valhuman02-21.png', 'valhuman01-13.png', 'valhuman04-03.png', 'valhuman03-05.png']\n"]}],"source":["train_horse_names = os.listdir(train_horse_dir)\n","print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n","\n","train_human_names = os.listdir(train_human_dir)\n","print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n","\n","validation_horse_names = os.listdir(validation_horse_dir)\n","print(f'VAL SET HORSES: {validation_horse_names[:10]}')\n","\n","validation_human_names = os.listdir(validation_human_dir)\n","print(f'VAL SET HUMANS: {validation_human_names[:10]}')"]},{"cell_type":"markdown","metadata":{"id":"HlqN5KbafhLI"},"source":["You can find out the total number of horse and human images in the directories:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"H4XHh2xSfgie","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704139455057,"user_tz":360,"elapsed":178,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"be1542e4-4dd3-470b-d98f-dd8595459b80"},"outputs":[{"output_type":"stream","name":"stdout","text":["total training horse images: 500\n","total training human images: 527\n","total validation horse images: 128\n","total validation human images: 128\n"]}],"source":["print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n","print(f'total training human images: {len(os.listdir(train_human_dir))}')\n","print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n","print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"]},{"cell_type":"markdown","metadata":{"id":"C3WZABE9eX-8"},"source":["Now take a look at a few pictures to get a better sense of what they look like. First, configure the `matplotlib` parameters:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"b2_Q0-_5UAv-","executionInfo":{"status":"ok","timestamp":1704139457751,"user_tz":360,"elapsed":158,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}}},"outputs":[],"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","# Parameters for our graph; we'll output images in a 4x4 configuration\n","nrows = 4\n","ncols = 4\n","\n","# Index for iterating over images\n","pic_index = 0"]},{"cell_type":"markdown","metadata":{"id":"xTvHzGCxXkqp"},"source":["Now, display a batch of 8 horse and 8 human pictures. You can rerun the cell to see a fresh batch each time:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Wpr8GxjOU8in","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ol_V1naG89TmEnYElZSby_r7_yhR6NJL"},"executionInfo":{"status":"ok","timestamp":1704139464107,"user_tz":360,"elapsed":4186,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"28c9149f-7ba6-4dae-c789-ed0290a108dc"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Set up matplotlib fig, and size it to fit 4x4 pics\n","fig = plt.gcf()\n","fig.set_size_inches(ncols * 4, nrows * 4)\n","\n","pic_index += 8\n","next_horse_pix = [os.path.join(train_horse_dir, fname)\n","                for fname in train_horse_names[pic_index-8:pic_index]]\n","next_human_pix = [os.path.join(train_human_dir, fname)\n","                for fname in train_human_names[pic_index-8:pic_index]]\n","\n","for i, img_path in enumerate(next_horse_pix+next_human_pix):\n","  # Set up subplot; subplot indices start at 1\n","  sp = plt.subplot(nrows, ncols, i + 1)\n","  sp.axis('Off') # Don't show axes (or gridlines)\n","\n","  img = mpimg.imread(img_path) #Read an image from a file into an array.\n","  plt.imshow(img) #Display data as an image (array like or PIL image), i.e., on a 2D regular raster.\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"5oqBkNBJmtUv"},"source":["## Building a Small Model from Scratch\n","\n","You will define the same model architecture as before:"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"qvfZg3LQbD-5","executionInfo":{"status":"ok","timestamp":1704139513280,"user_tz":360,"elapsed":4124,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","model = tf.keras.models.Sequential([\n","    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n","    # This is the first convolution\n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    # The second convolution\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The third convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The fourth convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The fifth convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # Flatten the results to feed into a DNN\n","    tf.keras.layers.Flatten(),\n","    # 512 neuron hidden layer\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])"]},{"cell_type":"markdown","metadata":{"id":"s9EaFDP5srBa"},"source":["You can review the network architecture and the output shapes with `model.summary()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZKj8392nbgP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704108726821,"user_tz":360,"elapsed":241,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"ac5b2f54-67b0-41f5-f7d5-53cca1bb6f9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 298, 298, 16)      448       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 149, 149, 16)      0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 73, 73, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 35, 35, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 33, 33, 64)        36928     \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 16, 16, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n","                                                                 \n"," max_pooling2d_4 (MaxPoolin  (None, 7, 7, 64)          0         \n"," g2D)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 3136)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1606144   \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 1704097 (6.50 MB)\n","Trainable params: 1704097 (6.50 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"PEkKSpZlvJXA"},"source":["You will also use the same compile settings as before:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8DHWhFP_uhq3","executionInfo":{"status":"ok","timestamp":1704139514556,"user_tz":360,"elapsed":211,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}}},"outputs":[],"source":["from tensorflow.keras.optimizers import RMSprop\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(learning_rate=0.001),\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"Sn9m9D3UimHM"},"source":["### Data Preprocessing\n","\n","Now you will setup the data generators. It will mostly be the same as last time but notice the additional code to also prepare the validation data. It will need to be instantiated separately and also scaled to have `[0,1]` range of pixel values."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ClebU9NJg99G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704139589282,"user_tz":360,"elapsed":210,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"0b4b1e1b-a28c-40cc-fc01-7863f8d5e6ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1027 images belonging to 2 classes.\n","Found 256 images belonging to 2 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","# Flow training images in batches of 128 using train_datagen generator - will need 8 epochs to train entire data\n","train_generator = train_datagen.flow_from_directory(\n","        './horse-or-human/',  # This is the source directory for training images\n","        target_size=(300, 300),  # All images will be resized to 300x300\n","        batch_size=128,\n","        # Since you use binary_crossentropy loss, you need binary labels\n","        class_mode='binary')\n","\n","# Flow validation images in batches of 32 using validation_datagen generator - will need 8 epochs to validate all images\n","validation_generator = validation_datagen.flow_from_directory(\n","        './validation-horse-or-human/',  # This is the source directory for validation images\n","        target_size=(300, 300),  # All images will be resized to 300x300\n","        batch_size=32,\n","        # Since you use binary_crossentropy loss, you need binary labels\n","        class_mode='binary')"]},{"cell_type":"markdown","metadata":{"id":"mu3Jdwkjwax4"},"source":["### Training\n","Now train the model for 15 epochs. Here, you will pass parameters for `validation_data` and `validation_steps`. With these, you will notice additional outputs in the print statements: `val_loss` and `val_accuracy`. Notice that as you train with more epochs, your training accuracy might go up but your validation accuracy goes down. This can be a sign of overfitting and you need to prevent your model from reaching this point."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Fb1_lgobv81m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704139768855,"user_tz":360,"elapsed":168394,"user":{"displayName":"Chaithra Kayce","userId":"06590399226295405413"}},"outputId":"33e78b51-f08d-43d7-a0c8-62e306961ec2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","8/8 [==============================] - 19s 2s/step - loss: 1.0308 - accuracy: 0.5484 - val_loss: 0.6886 - val_accuracy: 0.5000\n","Epoch 2/15\n","8/8 [==============================] - 11s 1s/step - loss: 0.6846 - accuracy: 0.6140 - val_loss: 0.6852 - val_accuracy: 0.5000\n","Epoch 3/15\n","8/8 [==============================] - 10s 1s/step - loss: 0.6323 - accuracy: 0.6808 - val_loss: 3.4304 - val_accuracy: 0.5000\n","Epoch 4/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.8384 - accuracy: 0.5851 - val_loss: 0.6046 - val_accuracy: 0.5742\n","Epoch 5/15\n","8/8 [==============================] - 8s 1s/step - loss: 0.5862 - accuracy: 0.6952 - val_loss: 1.3282 - val_accuracy: 0.5234\n","Epoch 6/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.4480 - accuracy: 0.7942 - val_loss: 1.0329 - val_accuracy: 0.7812\n","Epoch 7/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.4031 - accuracy: 0.8187 - val_loss: 1.3122 - val_accuracy: 0.7773\n","Epoch 8/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.5890 - accuracy: 0.7875 - val_loss: 0.6099 - val_accuracy: 0.8711\n","Epoch 9/15\n","8/8 [==============================] - 8s 989ms/step - loss: 0.2048 - accuracy: 0.9121 - val_loss: 1.4022 - val_accuracy: 0.8203\n","Epoch 10/15\n","8/8 [==============================] - 8s 1s/step - loss: 0.0962 - accuracy: 0.9655 - val_loss: 2.0624 - val_accuracy: 0.7500\n","Epoch 11/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.2027 - accuracy: 0.9266 - val_loss: 1.1071 - val_accuracy: 0.8516\n","Epoch 12/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.9618 - accuracy: 0.7875 - val_loss: 0.9473 - val_accuracy: 0.7969\n","Epoch 13/15\n","8/8 [==============================] - 8s 1s/step - loss: 0.3929 - accuracy: 0.8821 - val_loss: 1.1403 - val_accuracy: 0.6797\n","Epoch 14/15\n","8/8 [==============================] - 10s 1s/step - loss: 0.1295 - accuracy: 0.9499 - val_loss: 1.0539 - val_accuracy: 0.8398\n","Epoch 15/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.0496 - accuracy: 0.9789 - val_loss: 1.1857 - val_accuracy: 0.8516\n"]}],"source":["history = model.fit(\n","      train_generator,\n","      steps_per_epoch=8,\n","      epochs=15,\n","      verbose=1,\n","      validation_data = validation_generator,\n","      validation_steps=8)"]},{"cell_type":"markdown","metadata":{"id":"o6vSHzPR2ghH"},"source":["### Model Prediction\n","\n","Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human.\n","\n","_**Note:** Old versions of the Safari browser might have compatibility issues with the code block below. If you get an error after you select the images(s) to upload, you can consider updating your browser to the latest version. If not possible, please comment out or skip the code block below, uncomment the next code block and run it._"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoWp43WxJDNT"},"outputs":[],"source":["## NOTE: If you are using Safari and this cell throws an error,\n","## please skip this block and run the next one instead.\n","\n","import numpy as np\n","from google.colab import files\n","from tensorflow.keras.utils import load_img, img_to_array\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","\n","  # predicting images\n","  path = '/content/' + fn\n","  img = load_img(path, target_size=(300, 300))\n","  x = img_to_array(img)\n","  x /= 255\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a human\")\n","  else:\n","    print(fn + \" is a horse\")"]},{"cell_type":"markdown","metadata":{"id":"UJV8rdWU0NlM"},"source":["If you're using Safari and the cell above throws an error, you will need to upload the images(s) manually in their workspace.\n","\n","Instructions on how to upload image(s) manually in a Colab:\n","\n","1. Select the `folder` icon on the left `menu bar`.\n","2. Click on the `folder with an arrow pointing upwards` named `..`\n","3. Click on the `folder` named `tmp`.\n","4. Inside of the `tmp` folder, `create a new folder` called `images`. You'll see the `New folder` option by clicking the `3 vertical dots` menu button next to the `tmp` folder.\n","5. Inside of the new `images` folder, upload an image(s) of your choice, preferably of either a horse or a human. Drag and drop the images(s) on top of the `images` folder.\n","6. Uncomment and run the code block below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyIcglKE0MpY"},"outputs":[],"source":["# # CODE BLOCK FOR OLDER VERSIONS OF SAFARI\n","\n","# import numpy as np\n","# from tensorflow.keras.utils import load_img, img_to_array\n","# import os\n","\n","# images = os.listdir(\"/tmp/images\")\n","\n","# print(images)\n","\n","# for i in images:\n","#  print()\n","#  # predicting images\n","#  path = '/tmp/images/' + i\n","#  img = load_img(path, target_size=(300, 300))\n","#  x = img_to_array(img)\n","#  x /= 255\n","#  x = np.expand_dims(x, axis=0)\n","\n","#  images = np.vstack([x])\n","#  classes = model.predict(images, batch_size=10)\n","#  print(classes[0])\n","#  if classes[0]>0.5:\n","#    print(i + \" is a human\")\n","#  else:\n","#    print(i + \" is a horse\")"]},{"cell_type":"markdown","metadata":{"id":"-8EHQyWGDvWz"},"source":["### Visualizing Intermediate Representations\n","\n","As before, you can plot how the features are transformed as it goes through each layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5tES8rXFjux"},"outputs":[],"source":["import numpy as np\n","import random\n","from tensorflow.keras.utils import img_to_array, load_img\n","\n","# Define a new Model that will take an image as input, and will output\n","# intermediate representations for all layers in the previous model after\n","# the first.\n","successive_outputs = [layer.output for layer in model.layers[1:]]\n","visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n","\n","# Prepare a random input image from the training set.\n","horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n","human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n","img_path = random.choice(horse_img_files + human_img_files)\n","\n","img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n","x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n","x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n","\n","# Scale by 1/255\n","x /= 255\n","\n","# Run the image through the network, thus obtaining all\n","# intermediate representations for this image.\n","successive_feature_maps = visualization_model.predict(x)\n","\n","# These are the names of the layers, so you can have them as part of the plot\n","layer_names = [layer.name for layer in model.layers[1:]]\n","\n","# Display the representations\n","for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n","  if len(feature_map.shape) == 4:\n","\n","    # Just do this for the conv / maxpool layers, not the fully-connected layers\n","    n_features = feature_map.shape[-1]  # number of features in feature map\n","\n","    # The feature map has shape (1, size, size, n_features)\n","    size = feature_map.shape[1]\n","\n","    # Tile the images in this matrix\n","    display_grid = np.zeros((size, size * n_features))\n","    for i in range(n_features):\n","      x = feature_map[0, :, :, i]\n","      x -= x.mean()\n","      x /= x.std()\n","      x *= 64\n","      x += 128\n","      x = np.clip(x, 0, 255).astype('uint8')\n","\n","      # Tile each filter into this big horizontal grid\n","      display_grid[:, i * size : (i + 1) * size] = x\n","\n","    # Display the grid\n","    scale = 20. / n_features\n","    plt.figure(figsize=(scale * n_features, scale))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.imshow(display_grid, aspect='auto', cmap='viridis')"]},{"cell_type":"markdown","metadata":{"id":"j4IBgYCYooGD"},"source":["## Clean Up\n","\n","Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"651IgjLyo-Jx"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}